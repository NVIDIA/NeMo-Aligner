trainer:
  devices: 1
  num_nodes: 1
  accelerator: gpu
  precision: bf16 # 16, 32, or bf16

  deep_search:
    max_steps: -1
    val_check_interval: 1
    max_epochs: 1
    save_interval: ${.val_check_interval}

    gradient_clip_val: 1.0
    limit_val_batches: 0.1

    # how many batches per training step
    num_value_batches: 2
    num_policy_batches: 1

  # no need to change these
  logger: False # logger provided by exp_manager
  enable_checkpointing: False
  use_distributed_sampler: False
  max_time: null
  max_epochs: ${.deep_search.max_epochs}
  max_steps: ${.deep_search.max_steps}

pretrained_checkpoint:
  restore_from_path: null

mcts_data_file: null
 
model:
  mcts:
    C: 2 # weight for the UCB piror term
    num_searches: 800  # number of MCTS searches
    num_self_play_iterations: 2 # number of self play iterations

    rollout_micro_batch_size: 1 # batch size for each dp worker to handle
    num_rollouts: 1.0 # float(will be % of the dataset) or int

    temperature: 0.2  # use low temperature for more greedy search
    dirichlet_epsilon: 0.0  # weight for dirichelt noise added to the root state, turn off the dirichlet noise by setting this to 0
    dirichlet_alpha: 0.3 # parameter for dirichlet noise, the piror probability of the action happens 
    max_depth: 250  # maxium depth of the search tree

    top_k: 50
    end_strings: ["<extra_id_1>"]  # generation will stop when one of these tokens is generated

    train:
      value_weight: 1 # weight of the value portion of the loss
      policy_weight: 1 # weight of the policy portion of the loss

  inference:

    micro_batch_size: 16

    sampling_params:
      use_greedy: True # Whether or not to use sampling ; use greedy decoding otherwise
      top_k: ${model.mcts.top_k}  # The number of highest probability vocabulary tokens to keep for top-k-filtering.
      top_p: 0.9 # If set to float < 1, only the most probable tokens with probabilities that add up to top_p or higher are kept for generation.
      temperature: 1.0 # sampling temperature
      repetition_penalty: 1.2  # The parameter for repetition penalty. 1.0 means no penalty.
      add_BOS: False # add the bos token at the begining of the prompt
      all_probs: False  # whether return the log prob for all the tokens in vocab
      compute_logprob: False  # a flag used to compute logprob of all the input text, a very special case of running inference, default False
      end_strings: ${model.mcts.end_strings}

    length_params:
      max_length: 30
      min_length: 0
  
  offload_adam_states: False

  micro_batch_size: 1
  global_batch_size: 8

  mcore_gpt: True
  share_embeddings_and_output_weights: False

  # reward_model_type: binary_ranking # ["binary_ranking, "regression"]
  regression:
    num_attributes: 1 # dimension of regression head
    merge_attributes: False # whether to merge multiple attributes into a scalar
    attribute_weights: null # apply these weights to each attributes when merging them into a scalar
    loss_mask_val: -100 #  mask dimensions with this value when calculating MSE loss

  output_sequence: True  # Whether to output a single scalar or a sequence of scalars.
  use_avg_pool: False  # Whether to use avg pool to sum across the sequence dim in reward model
  force_head_dtype: float32  # enforce specific dtype for the final projection in the model head
  megatron_amp_O2: True
  tensor_model_parallel_size: 1
  pipeline_model_parallel_size: 1
  pipeline_model_parallel_split_rank: 0 # used for encoder and decoder model (0 for others)
  encoder_seq_length: 4096
  max_position_embeddings: ${model.encoder_seq_length}

  # parameters for value output
  value:
    max_position_embeddings: ${model.encoder_seq_length}
    seed: 1234
    num_layers: 2 # two layers
    tensor_model_parallel_size: ${model.tensor_model_parallel_size}

  # miscellaneous
  seed: 1234

  optim:
    name: distributed_fused_adam
    bucket_cap_mb: 200
    overlap_grad_sync: False
    contiguous_grad_buffer: True
    lr: 9e-7
    weight_decay: 0.1
    betas:
    - 0.9
    - 0.98
    sched:
      name: CosineAnnealing
      warmup_steps: 10
      constant_steps: 1000
      min_lr: 9e-8

  precision: ${trainer.precision}

  # define fields from the base model's config that should be ignored when merging with this config.
  overwrite_base_config:
      data:
        data_prefix: True

exp_manager:
  explicit_log_dir: /results
  exp_dir: null
  name: megatron_gpt_ppo_hybrid
  create_wandb_logger: False
  wandb_logger_kwargs:
    project: nemo_aligner_mcts
    name: gpt_10b
  resume_from_checkpoint: null # The path to a checkpoint file to continue the training, restores the whole state including the epoch, step, LR schedulers, apex, etc.
  resume_if_exists: True
  resume_ignore_no_checkpoint: True
  create_checkpoint_callback: False
  checkpoint_callback_params:
    monitor: val_accuracy
    save_top_k: 1
    mode: max
    always_save_nemo: False # saves nemo file during validation, not implemented for model parallel
    save_nemo_on_train_end: True # not recommended when training large models on clusters with short time limits
    filename: 'megatron_gpt-{step}-{consumed_samples}-{optimization_step}-{val_accuracy:.3f}'
    model_parallel_size: ${multiply:${model.tensor_model_parallel_size}, ${model.pipeline_model_parallel_size}}
