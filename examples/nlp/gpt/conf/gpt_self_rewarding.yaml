defaults:
  - optional tp_overlap@model.ub_tp_comm_overlap_cfg:

trainer:
  num_nodes: 8
  devices: 8
  accelerator: gpu
  precision: bf16
  
  # self-rewarding specific args
  self_rewarding:
    max_iterations: 3   # the number of iterations to conduct self-rewarding for, with the reference policy updated on each iteration
    max_epochs: 1       # the number of epochs to use per iteration
    max_steps: -1
    val_check_interval: 0.1
    save_interval: 100
    limit_train_batches: 1.0

    # how many GBS we loop over
    limit_val_batches: 1.0
    gradient_clip_val: 1.0

    # Speed-up training by accelerating inference stage using TRTLLM
    trt_llm:
      enable: False
      # reshard: False # reshard is not supported in Self-Rewarding

      # TRTLLM preallocates activation memory according to the number of input tokens
      # By default, assume the max input length is half of the model sequence length
      max_input_len: ${subtract:${model.encoder_seq_length}, ${model.spin.length_params.max_length}}

      model_type: gptnext # can be gptj, gptnext, llama, gemma, falcon

      # Save GPU memory by unloading and reloading the TRTLLM engine before and after the training stage
      # Reloading the engine incurs a constant time overhead
      unload_engine_train: False

  # do not change these
  logger: False # logger provided by exp_manager
  enable_checkpointing: False
  use_distributed_sampler: False
  max_time: null
  max_epochs: ${.self_rewarding.max_epochs}
  max_steps: ${.self_rewarding.max_steps}

exp_manager:
  explicit_log_dir: /results
  exp_dir: null
  name: megatron_gpt
  max_time_per_run: null
  create_wandb_logger: False
  wandb_logger_kwargs:
    project: nemo_aligner_self_rewarding
    name: rlhf_gpt3_self_rewarding
  resume_from_checkpoint: null # The path to a checkpoint file to continue the training, restores the whole state including the epoch, step, LR schedulers, apex, etc.
  resume_if_exists: True
  resume_ignore_no_checkpoint: True
  create_checkpoint_callback: True
  checkpoint_callback_params:
    monitor: val_loss
    save_top_k: 3
    mode: min
    always_save_nemo: False # saves nemo file during validation, not implemented for model parallel
    save_nemo_on_train_end: True # not recommended when training large models on clusters with short time limits
    filename: 'megatron_gpt--{${.monitor}:.3f}-{step}-{consumed_samples}-{iteration}-{epoch}'
    model_parallel_size: ${multiply:${model.tensor_model_parallel_size}, ${model.pipeline_model_parallel_size}}

pretrained_checkpoint:
  restore_from_path: null

model:  
  mcore_gpt: True
  micro_batch_size: 1
  global_batch_size: 64
  megatron_amp_O2: True

  # This config section is named `spin` because the Self-Rewarding algorithm re-uses the SPIN model class.
  spin:
    log_prob_forward_micro_batch_size: ${divide:${.rollout_micro_batch_size}, 2}  # divide by 2 here because we have chosen/rejected per sample
    rollout_micro_batch_size: 32
    ref_policy_kl_penalty: 0.1  # Can also be a list of elements of size max_iterations, where each element will be the KL penalty used for that iteration
    offload_adam_states: True
    num_responses_to_gen: 4  # number of responses to generate per prompt
    num_evals_to_average: 3  # number of times to evaluate each response via LLM-as-a-judge (and then take the average score)
    length_control: 0.0      # the length control rho parameter to use as per the paper's implementation
    use_meta_judge: False    # whether or not to use the meta rewarding algorithm by generating DPO pairs for the LLM judge itself
    meta_judge_pcnt: -1      # percentage of each GBS to dedicate to meta judge samples (if `use_meta_judge=True`)
    first_iteration_sft: False  # whether the first iteration should be the SFT loss instead of DPO
    preference_average_log_probs: False # whether normalizing log probs according to the sequence length in preference_loss
    sft_average_log_probs: ${.preference_average_log_probs} # whether normalizing log probs according to the sequence length in sft_loss
    gt_reward_scale: 1. # the scale of the rewards in RPO
    preference_loss: dpo # the preference loss, we support dpo, ipo, rpo_sq, rpo_bwd_kl, rpo_fwd_kl
    preference_loss_weight: 1 # the coefficient of the preference loss
    sft_loss_weight: 0 # the coefficient of the SFT loss
    judge_score_low: 0  # the lowest score which the LLM-as-a-judge can award, as per your llm_judge_prompt
    judge_score_high: 5  # the highest score which the LLM-as-a-judge can award, as per your llm_judge_prompt
    meta_max_relative_pcnt: 0.4  # the largest percentage that any one score category can be for the purposes of training (e.g. no score can be more than 40% of the total training data)

    judge_reward_regex: "(?i)(?:Score|Points): {{ reward }}"
    meta_judge_reward_regex: "(?i)Winner: (?:Judgement|Judgment) {{ reward }}"

    # Current default judge prompt is copied from https://arxiv.org/abs/2401.10020
    llm_judge_prompt: |
      ${model.data.chat_prompt_tokens.system_turn_start}System
      
      ${model.data.chat_prompt_tokens.turn_start}User
      Review the user's question and the corresponding response using the additive 5-point
      scoring system described below. Points are accumulated based on the satisfaction of each
      criterion:
      - Add 1 point if the response is relevant and provides some information related to
      the user's inquiry, even if it is incomplete or contains some irrelevant content.
      - Add another point if the response addresses a substantial portion of the user's question,
      but does not completely resolve the query or provide a direct answer.
      - Award a third point if the response answers the basic elements of the user's question in a
      useful way, regardless of whether it seems to have been written by an AI Assistant or if it
      has elements typically found in blogs or search results.
      - Grant a fourth point if the response is clearly written from an AI Assistant's perspective,
      addressing the user's question directly and comprehensively, and is well-organized and
      helpful, even if there is slight room for improvement in clarity, conciseness or focus.
      - Bestow a fifth point for a response that is impeccably tailored to the user's question
      by an AI Assistant, without extraneous information, reflecting expert knowledge, and
      demonstrating a high-quality, engaging, and insightful answer.
      
      <prompt>{{ prompt }}</prompt>
      <response>{{ response }}</response>
      
      After examining the user's instruction and the response:
      - Briefly justify your total score, up to 100 words.
      - Conclude with the score using the format: "Score: <total points>"
      Remember to assess from the AI Assistant perspective, utilizing web search knowledge as
      necessary. To evaluate the response in alignment with this additive scoring model, we'll
      systematically attribute points based on the outlined criteria.
      ${model.data.chat_prompt_tokens.turn_start}Assistant

    # Current default meta-judge prompt is copied from https://arxiv.org/abs/2407.19594
    llm_meta_judge_prompt: |
      ${model.data.chat_prompt_tokens.system_turn_start}System
      
      ${model.data.chat_prompt_tokens.turn_start}User
      Review the user's question and the corresponding response, along with two judgments.
      Determine which judgment is more accurate according to the rubric provided below. The
      rubric used for the initial judgments is as follows:
      - Add 1 point if the response is relevant and provides some information related to
      the user's inquiry, even if it is incomplete or contains some irrelevant content.
      - Add another point if the response addresses a substantial portion of the user's question,
      but does not completely resolve the query or provide a direct answer.
      - Award a third point if the response answers the basic elements of the user's question in a
      useful way, regardless of whether it seems to have been written by an AI Assistant or if it
      has elements typically found in blogs or search results.
      - Grant a fourth point if the response is clearly written from an AI Assistant's perspective,
      addressing the user's question directly and comprehensively, and is well-organized and helpful,
      even if there is slight room for improvement in clarity, conciseness or focus.
      - Bestow a fifth point for a response that is impeccably tailored to the user's question
      by an AI Assistant, without extraneous information, reflecting expert knowledge, and
      demonstrating a high-quality, engaging, and insightful answer.
      
      <prompt>{{ prompt }}<prompt>
      <response>{{ response }}<response>
      <judgement A>{{ judgement_a }}<judgement A>
      <judgement B>{{ judgement_b }}<judgement B>
      
      After examining the original question, response, and both judgments:
      - Explain which judgment is more accurate according to the original rubric and why.
      Consider factors such as adherence to the rubric, accuracy in evaluating the response, and
      consistency in applying the criteria.
      - Conclude with a clear statement of which judgment is better using the format: "Winner: [Judgement A | Judgement B]"
      ${model.data.chat_prompt_tokens.turn_start}Assistant


    # params for generation
    sampling_params:
      use_greedy: False
      temperature: 1.0
      top_k: 0
      top_p: 1.0
      repetition_penalty: 1.0
      add_BOS: False
      all_probs: False
      compute_logprob: False
      end_strings: ["<|endoftext|>", "<extra_id_1>"]

    # length argument for autoregressive sampling
    # max length means max amount of tokens to generate
    length_params:
      max_length: ${int_div:${model.encoder_seq_length}, 2}
      min_length: 1
  
  #encoder_seq_length: 4096
  #max_position_embeddings: ${model.encoder_seq_length}

  # miscellaneous
  seed: 1234

  optim:
    name: distributed_fused_adam
    bucket_cap_mb: 200
    overlap_grad_sync: False
    contiguous_grad_buffer: True
    lr: 9e-6
    weight_decay: 0.1 
    betas: 
    - 0.9
    - 0.98
    sched:
      name: CosineAnnealing
      warmup_steps: 10
      constant_steps: 1000
      min_lr: 9e-7

  precision: ${trainer.precision}

  data:
    chat: True # whether to use chatbot data or not
    chat_prompt_tokens:  # special tokens for the chat prompts, a dictionary of {token_type: token}. note that some tokenizer may combine the characters at the junction between {end_of_turn}{turn_start}. e.g. '<im end><im start>', the '><' sometimes is merged to be a single token. This is not supported, try to avoid
      system_turn_start: "<extra_id_0>"
      turn_start: "<extra_id_1>"
      label_start: "<extra_id_2>"
      end_of_turn: "\x0A"  # \0x0A is '\n'
      end_of_name: "\x0A"  # \0x0A is '\n'

    sample: False # create the index mapping files for the sample data, so max_steps * global_batch_size can be larger than the dataset size
    num_workers: 0
    dataloader_type: single  # only supports single
    reset_position_ids: False # Reset position ids after end-of-document token
    reset_attention_mask: False # Reset attention mask after end-of-document token
    eod_mask_loss: False # Mask loss for the end of document tokens
    train_ds:
      # Example of how to specify paths to multiple datasets
      # file_names:
      #   - /path/to/squad.jsonl
      #   - /path/to/mnli.jsonl
      #   - /path/to/boolq.jsonl
      # Example of how each dataset is formatted
      # {'input': 'John von Neumann\nVon Neumann made fundamental contributions .... Q: What did the math of artificial viscosity do?', 'output': 'smoothed the shock transition without sacrificing basic physics'}
      file_path: ??? # Path to a JSONL file corresponding to the source data. Data format is identical to validation_ds.
      shuffle: True
      memmap_workers: null
      max_seq_length: ${model.encoder_seq_length}
      min_seq_length: 1
      drop_last: True
      # Example of how to specify concat_sampling_probabilities
      # concat_sampling_probabilities:
      #   - 0.5
      #   - 0.25
      #   - 0.25
      label_key: 'output'
      add_eos: False
      add_sep: False
      add_bos: False
      truncation_field: "context_ids" # # Can be multiple keys separated with ',' Options: keys in prompt_template
      index_mapping_dir: null # Path to a directory to write index mapping files.
      prompt_template: null # fstring to use for assistant prompt. Example: "Q: {input}\nA: {output}"
      hf_dataset: True # Whether to load the json file with the HuggingFace dataset. otherwise, will load the jsonl file with the JSONLMemMapDataset.
      truncation_method: 'right' # Truncation from which position, Options: ['left', 'right']

    validation_ds:
      file_path: ??? # Path to a JSONL file corresponding to the source data. Data format is identical to validation_ds.
      global_batch_size: ${model.global_batch_size}
      micro_batch_size: ${model.micro_batch_size}
      shuffle: False
      memmap_workers: ${model.data.train_ds.memmap_workers}
      max_seq_length: ${model.data.train_ds.max_seq_length}
      min_seq_length: 1
      drop_last: True
      label_key: ${model.data.train_ds.label_key}
      add_eos: ${model.data.train_ds.add_eos}
      add_sep: ${model.data.train_ds.add_sep}
      add_bos: ${model.data.train_ds.add_bos}
      truncation_field: ${model.data.train_ds.truncation_field} # Options: keys in prompt_template
      index_mapping_dir: null # Path to a directory to write index mapping files.
      prompt_template: ${model.data.train_ds.prompt_template} # fstring to use for assistant prompt. Example: "Q: {input}\nA: {output}"
      hf_dataset: True # Whether to load the json file with the HuggingFace dataset. otherwise, will load the jsonl file with the JSONLMemMapDataset.
      truncation_method: 'right' # Truncation from which position, Options: ['left', 'right']
      output_original_text: True  # needed for the proper metrics support

  # define fields from the base model's config that should be ignored when merging with this config.
  overwrite_base_config:
    data:
      train_ds:
        file_path: True
      validation_ds:
        file_path: True
