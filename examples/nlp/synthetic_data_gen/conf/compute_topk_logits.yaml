name: megatron_gpt_infer_topk_logits

start_from_idx: null
end_at_idx: null
output_path: null 
top_k: 100

trainer:
  num_nodes: 1
  devices: 1
  accelerator: gpu
  precision: bf16

  # do not change these
  logger: False # logger provided by exp_manager
  enable_checkpointing: False
  use_distributed_sampler: False
  max_time: null
  max_epochs: 1

pretrained_checkpoint:
  restore_from_path: null

model:
  
  mcore_gpt: True
  micro_batch_size: 1
  global_batch_size: 1
  megatron_amp_O2: True

  seed: 1234
  tensor_model_parallel_size: 1 # intra-layer model parallelism
  pipeline_model_parallel_size: 1 # inter-layer model parallelism
  restore_from_path: ??? # Path to an existing p-tuned/prompt tuned .nemo model you wish to add new tasks to or run inference with
  resume_from_checkpoint: null # The path to a checkpoint file to continue the training, restores the whole state including the epoch, step, LR schedulers, apex, etc.
  save_nemo_on_validation_end: True # Saves an inference ready .nemo file every time a checkpoint is saved during training.
  sync_batch_comm: False
  encoder_seq_length: 4096  # the sequence length of the encoder model, it will be overwriten by loaded GPT model

  ## Sequence Parallelism
  # Makes tensor parallelism more memory efficient for LLMs (20B+) by parallelizing layer norms and dropout sequentially
  # See Reducing Activation Recomputation in Large Transformer Models: https://arxiv.org/abs/2205.05198 for more details.
  sequence_parallel: False

  ## Activation Checkpoint
  activations_checkpoint_granularity: null # 'selective' or 'full'
  activations_checkpoint_method: null # 'uniform', 'block', not used with 'selective'
  # 'uniform' divides the total number of transformer layers and checkpoints the input activation
  # of each chunk at the specified granularity
  # 'block' checkpoints the specified number of layers per pipeline stage at the specified granularity
  activations_checkpoint_num_layers: null # not used with 'selective'
  activations_checkpoint_layers_per_pipeline: null
  # This feature is valid only when used with pipeline-model-parallelism. More details in megatron_gpt_config.yaml.
  answer_only_loss: False # not used right now
  gradient_as_bucket_view: False
  seq_len_interpolation_factor: null # if not None, seq_len_interpolation_factor will match the base model's value
  use_flash_attention: null # if not None, will match the base model's value

  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0

data:
  chat: False # whether use chatbot data or not
  chat_prompt_tokens:  # special tokens for the chat prompts, a dictionary of {token_type: token}. note that some tokenizer may combine the characters at the junction between {end_of_turn}{turn_start}. e.g. '<im end><im start>', the '><' sometimes is merged to be a single token. This is not supported, try to avoid
    system_turn_start: "<extra_id_0>"
    turn_start: "<extra_id_1>"
    label_start: "<extra_id_2>"
    end_of_turn: "\x0A"  # \0x0A is '\n'
    end_of_name: "\x0A"  # \0x0A is '\n'
  sample: False # create the index mapping files for the sample data, so max_steps * global_batch_size can be larger than the dataset size
  num_workers: 0
  dataloader_type: single  # only supports single
  data:
    # Example of how to specify paths to multiple datasets
    # file_names:
    #   - /path/to/squad.jsonl
    #   - /path/to/mnli.jsonl
    #   - /path/to/boolq.jsonl
    # Example of how each dataset is formatted
    # {'input': 'John von Neumann\nVon Neumann made fundamental contributions .... Q: What did the math of artificial viscosity do?', 'output': 'smoothed the shock transition without sacrificing basic physics'}
    file_path: ??? # Path to a JSONL file corresponding to the source data. Data format is identical to validation_ds.
    shuffle: True
    memmap_workers: null
    max_seq_length: ${model.encoder_seq_length}
    min_seq_length: 1
    drop_last: True  # note that `False` is not currently supported
    # Example of how to specify concat_sampling_probabilities
    # concat_sampling_probabilities:
    #   - 0.5
    #   - 0.25
    #   - 0.25
    label_key: 'output'
    add_eos: True
    add_sep: False
    add_bos: False
    truncation_field: "input" # # Can be multiple keys separated with ',' Options: keys in prompt_template
    index_mapping_dir: null # Path to a directory to write index mapping files.
    prompt_template: "{input} {output}" # fstring to use for assistant prompt. Example: "Q: {input}\nA: {output}"
    hf_dataset: False # Whether to load the json file with the HuggingFace dataset. otherwise, will load the jsonl file with the JSONLMemMapDataset.
    truncation_method: 'right' # Truncation from which position, Options: ['left', 'right']
